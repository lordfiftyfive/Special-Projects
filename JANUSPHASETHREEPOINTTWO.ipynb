{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JANUSPHASETHREEPOINTTWO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lordfiftyfive/Special-Projects/blob/master/JANUSPHASETHREEPOINTTWO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yk2npNav97l",
        "colab_type": "code",
        "outputId": "7289c633-7d22-40fc-fc57-45a7600fa47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " #!pip install --upgrade tf-nightly tfp-nightly\n",
        "#!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.0.0-beta1\n",
        "#!pip install tensorflow==2.0.0rc0\n",
        "#!pip install tensorflow-probability\n",
        "#!pip install gaussian_processes\n",
        "!pip install quandl\n",
        "#!pip install rpy2\n",
        "!pip install bootstrapped\n",
        "!pip install hurst\n",
        "!pip install stldecompose\n",
        "!pip install GPyOpt\n",
        "!pip install parameter-sherpa\n",
        "\n",
        "#!pip install tensorflow-probability==0.8.0\n",
        "#!pip install gpflow==1.5.1\n",
        "#!pip install gpflow==2.0.0rc1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0a0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.18.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.27.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.12.0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (0.3.3)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.14.0a20190301)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0a0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0a0) (46.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0) (1.0.0)\n",
            "Collecting tensorflow-probability==0.7.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/a5/c4dac165347b939a59406b33510f0ecc37cef296f078d6517b8643b0bb13/tensorflow_probability-0.7.0rc0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0rc0) (1.18.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0rc0) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0rc0) (1.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0rc0) (4.4.2)\n",
            "\u001b[31mERROR: tensorflow-gan 2.0.0 has requirement tensorflow-probability>=0.7, but you'll have tensorflow-probability 0.7.0rc0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensor2tensor 1.14.1 has requirement tensorflow-probability==0.7.0, but you'll have tensorflow-probability 0.7.0rc0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-probability\n",
            "  Found existing installation: tensorflow-probability 0.8.0rc0\n",
            "    Uninstalling tensorflow-probability-0.8.0rc0:\n",
            "      Successfully uninstalled tensorflow-probability-0.8.0rc0\n",
            "Successfully installed tensorflow-probability-0.7.0rc0\n",
            "Requirement already satisfied: quandl in /usr/local/lib/python3.6/dist-packages (3.5.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from quandl) (8.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from quandl) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from quandl) (1.18.2)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from quandl) (2.21.0)\n",
            "Requirement already satisfied: inflection>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from quandl) (0.3.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from quandl) (2.8.1)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.6/dist-packages (from quandl) (0.25.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2019.11.28)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.14->quandl) (2018.9)\n",
            "Requirement already satisfied: bootstrapped in /usr/local/lib/python3.6/dist-packages (0.0.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from bootstrapped) (1.18.2)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from bootstrapped) (0.25.3)\n",
            "Requirement already satisfied: matplotlib>=1.5.3 in /usr/local/lib/python3.6/dist-packages (from bootstrapped) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.1->bootstrapped) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.1->bootstrapped) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.3->bootstrapped) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.3->bootstrapped) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.3->bootstrapped) (2.4.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.18.1->bootstrapped) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.3->bootstrapped) (46.0.0)\n",
            "Requirement already satisfied: hurst in /usr/local/lib/python3.6/dist-packages (0.0.5)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.6/dist-packages (from hurst) (0.25.3)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from hurst) (1.18.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->hurst) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->hurst) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.18->hurst) (1.12.0)\n",
            "Requirement already satisfied: stldecompose in /usr/local/lib/python3.6/dist-packages (0.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stldecompose) (1.18.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from stldecompose) (0.10.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stldecompose) (0.25.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stldecompose) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stldecompose) (3.2.1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->stldecompose) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stldecompose) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->stldecompose) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stldecompose) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stldecompose) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stldecompose) (2.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.0->statsmodels->stldecompose) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->stldecompose) (46.0.0)\n",
            "Requirement already satisfied: GPyOpt in /usr/local/lib/python3.6/dist-packages (1.2.6)\n",
            "Requirement already satisfied: GPy>=1.8 in /usr/local/lib/python3.6/dist-packages (from GPyOpt) (1.9.9)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from GPyOpt) (1.18.2)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from GPyOpt) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from GPy>=1.8->GPyOpt) (1.12.0)\n",
            "Requirement already satisfied: paramz>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from GPy>=1.8->GPyOpt) (0.9.5)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.6/dist-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt) (4.4.2)\n",
            "Requirement already satisfied: parameter-sherpa in /usr/local/lib/python3.6/dist-packages (1.0.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from parameter-sherpa) (3.2.1)\n",
            "Requirement already satisfied: GPyOpt>=1.2.5 in /usr/local/lib/python3.6/dist-packages (from parameter-sherpa) (1.2.6)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from parameter-sherpa) (1.18.2)\n",
            "Requirement already satisfied: enum34 in /usr/local/lib/python3.6/dist-packages (from parameter-sherpa) (1.1.10)\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from parameter-sherpa) (0.25.3)\n",
            "Requirement already satisfied: flask>=0.12.2 in /usr/local/lib/python3.6/dist-packages (from parameter-sherpa) (1.1.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from parameter-sherpa) (0.22.2.post1)\n",
            "Requirement already satisfied: pymongo>=3.5.1 in /usr/local/lib/python3.6/dist-packages (from parameter-sherpa) (3.10.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from parameter-sherpa) (1.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->parameter-sherpa) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->parameter-sherpa) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->parameter-sherpa) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->parameter-sherpa) (1.1.0)\n",
            "Requirement already satisfied: GPy>=1.8 in /usr/local/lib/python3.6/dist-packages (from GPyOpt>=1.2.5->parameter-sherpa) (1.9.9)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.3->parameter-sherpa) (2018.9)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=0.12.2->parameter-sherpa) (7.1.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=0.12.2->parameter-sherpa) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=0.12.2->parameter-sherpa) (2.11.1)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=0.12.2->parameter-sherpa) (1.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->parameter-sherpa) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->parameter-sherpa) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->parameter-sherpa) (46.0.0)\n",
            "Requirement already satisfied: paramz>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (0.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=0.12.2->parameter-sherpa) (1.1.1)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.6/dist-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MctkY11foHOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!git clone git://github.com/statsmodels/statsmodels.git\n",
        "#!pip install git+https://github.com/statsmodels/statsmodels\n",
        "#install-gpflow-20-beta-version."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsDjEvOD5SZX",
        "colab_type": "text"
      },
      "source": [
        "possible additional data\n",
        "\n",
        "https://usafacts.org/metrics/33937?regions=US&year=2016\n",
        "\n",
        "https://usafacts.org/metrics/55121?regions=US&year=2018"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYBxgSRmSueO",
        "colab_type": "code",
        "outputId": "d33a25a2-baa7-4371-d00b-c1170b4f26d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        }
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import quandl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import rpy2\n",
        "#from rpy2.robjects.packages import importr\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "#from keras.layers import Input, Dense, Activation, Flatten, BatchNormalization\n",
        "#from tensorflow_probability import sts\n",
        "import pandas as pd\n",
        "import bootstrapped.bootstrap as bs\n",
        "import bootstrapped.stats_functions as bs_stats\n",
        "from scipy.stats import ttest_ind, ttest_ind_from_stats,chisquare, mstats\n",
        "import seaborn as sns\n",
        "import statsmodels.api\n",
        "#from statsmodels.tsa.seasonal import STL\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from hurst import compute_Hc, random_walk\n",
        "import scipy.integrate as integrate\n",
        "from stldecompose import decompose\n",
        "import sherpa\n",
        "#import gpflow\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d7e6156c962c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_probability/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# pylint: enable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbijectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0medward2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzipf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZipf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfill_triangular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfill_triangular_inverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatrix_diag_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'fill_triangular'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMtN0kBGKZ_s",
        "colab_type": "text"
      },
      "source": [
        "SEE THIS LINK FOR JANUS PHASE THREE\n",
        "\n",
        "https://github.com/GPflow/GPflow/issues/1007"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVR8HB-uSLOx",
        "colab_type": "code",
        "outputId": "2bb9aa2f-bcc2-42cd-e03c-bca7f942c764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Spyder Editor\n",
        "\n",
        "This is a temporary script file.\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Importing libraries\n",
        "\"\"\"\n",
        "#import os\n",
        "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\"\"\"\n",
        "import keras\n",
        "from keras.layers import Input, Dense, Activation, Flatten, BatchNormalization, RNN, LSTM\n",
        "import numpy as np\n",
        "#from numpy import array\n",
        "from keras.models import load_model, Model, model_from_json, Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "#import datetime\n",
        "#import requests\n",
        "#import sys\n",
        "#from time import time, sleep\n",
        "#import time as systime\n",
        "import quandl\n",
        "#import math\n",
        "#import collections, itertools\n",
        "import tensorflow as tf\n",
        "#from sklearn import preprocessing\n",
        "#from sklearn.preprocessing import Imputer, StandardScaler, Normalizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import mstats\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_ind, ttest_ind_from_stats,chisquare\n",
        "from sklearn.decomposition import PCA\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "@misc{,\n",
        "  title={EconomicGrowthPredictor},\n",
        "  author={Subarno},\n",
        "  year={2018},\n",
        "  \n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# in supervised clustering samples are in rows and features are in columns\n",
        "\n",
        "\"\"\"\n",
        "unsupervised learning can be used to find hidden patterns data \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Getting data\n",
        "\"\"\"\n",
        "quandl.ApiConfig.api_key = 'DNMZo2iRzVENxpxqHBKF'\n",
        "\n",
        "#te.getCalendarData(country=['united states', 'china'], category=['imports','exports'],\n",
        "                   #initDate='2017-06-07', endDate='2017-12-31',\n",
        "                   #output_type='df')\n",
        "datafour = quandl.get(\"YALE/SPCOMP\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", transform=\"rdiff\", collapse=\"quarterly\", start_date=\"1959-09-30\", end_date=\"2018-12-31\")# earliest date is 1960-06-30 #quandl.get(\"YALE/CPIQ\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", collapse=\"quarterly\", start_date=\"1970-12-31\", end_date=\"2016-03-31\")#quandl.get(\"UNAE/GDPCD_USA\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", end_date=\"2016-12-31\")# quandl.get(\"UNAE/GDPCD_USA\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", end_date=\"2016-12-31\")\n",
        "Data_to_predict = quandl.get(\"FRBP/GDPPLUS_042619\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", collapse=\"quarterly\")#quandl.get(\"FRBP/GDPPLUS_042619\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", transform=\"rdiff\")#quandl.get(\"FRBP/GDPPLUS\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", collapse=\"quarterly\", start_date=\"1960-06-30\")\n",
        "datafive = quandl.get(\"FRED/PCETRIM1M158SFRBDAL\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", collapse=\"quarterly\", start_date=\"1977-02-01\",end_date=\"2016-03-31\")#quandl.get(\"FRED/VALEXPUSM052N\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", transform=\"rdiff\", collapse=\"quarterly\", start_date=\"1960-09-30\")#quandl.get(\"WWDI/USA_NE_GDI_TOTL_CD\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", start_date=\"1970-12-31\")\n",
        "Data_To_predict = Data_to_predict.values\n",
        "DESPAIR = quandl.get(\"USMISERY/INDEX\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", transform=\"rdiff\", collapse=\"quarterly\", start_date=\"1959-09-30\", end_date=\"2018-12-31\")    \n",
        "Debt_data_change_of_change = quandl.get(\"FRED/NCBDSLQ027S\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", transform=\"rdiff\",collapse=\"quarterly\",start_date=\"1959-06-30\",end_date=\"2018-12-31\")\n",
        "      \n",
        "DataSix = quandl.get(\"FRED/ROWFDIQ027S\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", transform=\"rdiff\", collapse=\"quarterly\", start_date=\"1959-06-30\",end_date=\"2018-12-31\")\n",
        "print(DataSix)\n",
        "#Data_To_Predict = np.matrix(Data_To_predict) \n",
        "print(\"datasix\")\n",
        "#print(Data_To_Predict)\n",
        "#print(\"input\")\n",
        "#print(Datafour)\n",
        "#early_stop = EarlyStopping(monitor='loss',patience=5, verbose=1)\n",
        "data_to_predict = Data_To_predict#[-47::]#np.reshape(Data_TO_predict, (9,6)\n",
        "\n",
        "\"\"\"\n",
        "#Ask if my model is making good predictions or whether the predicitons are due to the loss of information from the nornmalization procedure of the outputs\n",
        "\n",
        "#create a plot of the data before this ß\n",
        "#split_date= pd.Timestamp('01-01-2011')\n",
        "#train = df.loc(:split_date,[''])\n",
        "#test = df.loc(split_date:, [''])\n",
        "\n",
        "\"\"\"\n",
        "#data preprocessing\n",
        "\n",
        "trainingtarget = data_to_predict#normalizer.transform(data_to_predict)\n",
        "#mstats.winsorize()\n",
        "Scalar = MinMaxScaler(feature_range=(0,1))\n",
        "\t\n",
        "#trainingtarget = Scalar.fit_transform(trainingtarget)\n",
        "    \n",
        "    \n",
        "#datafour = preprocessing.scale(datafour)\n",
        "#trainingtarget = preprocessing.scale(trainingtarget)\n",
        "datasix = pd.DataFrame(DataSix)\n",
        "datafour = pd.DataFrame(datafour)\n",
        "datasix = datasix.drop(datasix.index[0])\n",
        "print(\"data six\")\n",
        "print(datasix)\n",
        "#\n",
        " #class statsmodels.tsa.seasonal.STL(endog, period=None, seasonal=7, trend=None, low_pass=None, seasonal_deg=0, trend_deg=0, low_pass_deg=0, robust=False, seasonal_jump=1, trend_jump=1, low_pass_jump=1)¶\n",
        "datafour = pd.concat([datafour, Debt_data_change_of_change,DESPAIR], axis=1)\n",
        "#datafour = datafour.fillna(0)\n",
        "data_to_predict = pd.DataFrame(data_to_predict)\n",
        "#z = pd.concat([datafour,data_to_predict])\n",
        "#print(z.ndim)\n",
        "#z = Scalar.fit(z)\n",
        "#z = np.array(z)\n",
        "#z = np.vsplit(z,7)\n",
        "        \n",
        "#datafour = z[0]\n",
        "     \n",
        "#trainingtarget = z[1]\n",
        "    \n",
        "#z = pd.concat(datafour,trainingtarget)\n",
        "#z = Scalar.fit_transform(z)\n",
        "#print(z.shape())\n",
        "#print(z)\n",
        "print(datafour)\n",
        "print(trainingtarget)\n",
        "pca = PCA()\n",
        "a = decompose(trainingtarget)#seasonal_decompose(trainingtarget,model='additive',freq=1)\n",
        "a.plot()\n",
        "print(trainingtarget)\n",
        "plt.plot(trainingtarget)\n",
        "#trainingtarget = a.trend\n",
        "datafour = Scalar.fit_transform(datafour)\n",
        "datafour = pca.fit_transform(datafour)\n",
        "#datafour = normalizer.fit(datafour)\n",
        "trainingtarget = Scalar.fit_transform(trainingtarget)\n",
        "y = trainingtarget\n",
        "X = datafour\n",
        "#trainingtarget = pd.DataFrame(trainingtarget)\n",
        "#trainingtarget = pd.DataFrame(trainingtarget)\n",
        "#time_index = pd.date_range(start=1959.6, end=2018.8, periods=237)#np.linspace(1959.6,2018.8,237)#np.linspace(0, 10, N, endpoint=True)\n",
        "#trainingtarget = trainingtarget.set_index(time_index)\n",
        "\n",
        "\n",
        "\n",
        "#trainingtarget = statsmodels.tsa.seasonal.STL(trainingtarget,period=None)\n",
        "#Reminder: look into possibility that the DATES column is being included into datafour\n",
        "\n",
        "#inputOne = len(dataThree)\n",
        "#print(inputOne)\n",
        "#print(dataThree)\n",
        "#x_train = Scalar.fit_transform(datafour[:220:])\n",
        "#y_train = Scalar.fit_transform(trainingtarget[:220:])\n",
        "#x_train = pca.fit(x_train)\n",
        "#x_test = Scalar.transform(datafour[220::]) \n",
        "\n",
        "print(\"shape of x\")\n",
        "print(X.shape)\n",
        "\n",
        "y = trainingtarget#[:, None]\n",
        "#Xshape = np.arange(dataThree).reshape(86, 1)\n",
        "#xshape.flat(86)\n",
        "#ConsolidatedInput = pd.merge(dataThree,dataTwo,dataOne)\n",
        "#b = dataThree.flatten('K')\n",
        "\t\n",
        "#xShape = X[:,None]#datafour.shape[1]\n",
        "#print(xShape)\n",
        "#X=np.reshape(X.shape[1],X.shape[0]) \n",
        "#datafour = tf.cast(datafour, tf.float32)\n",
        "#trainingtarget = tf.cast(trainingtarget, tf.float32)    \n",
        "#Deep learning algorithm\n",
        "\t\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "inputs = tf.keras.Input(shape=(1,13))\n",
        "first  = tf.keras.layers.LSTM(10,activation='relu',kernel_initializer='ones', use_bias=False)(inputs)\n",
        "\n",
        "u = tf.keras.layers.Dense(250,activation='relu',kernel_initializer='ones',use_bias=False)(first)\n",
        "b = tf.keras.layers.BatchNormalization()(u)\n",
        "u = tf.keras.layers.Dense(500, activation='relu',kernel_initializer='ones',use_bias=False)(b)\n",
        "u = tf.keras.layers.Dense(10, activation='relu')(u)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(u)\n",
        "Model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "#Adadelta\n",
        "Model.compile(optimizer='Adagrad',\n",
        "        loss='MSLE',\n",
        "        metrics=['accuracy'])\n",
        "\t\n",
        "Model.fit(X,Y,epochs=8, verbose=1,validation_split=0.2)#, callbacks=[early_stop])\n",
        "\n",
        "Prediction = Model.predict(X[189::])\n",
        "print(Prediction)\n",
        "print(Prediction.shape)\n",
        "u = 219\n",
        "Model.summary()\n",
        "\"\"\"\n",
        "#a = model.evaluate(trainingtarget[180::], Prediction, verbose=0)\n",
        "#.​save_weights(​'weights-init.h5'​)\n",
        "#trainingtarget = trainingtarget#[:-15:]\n",
        "#trainingtarget = normalizer.transform(trainingtarget)\n",
        "#blue is prediction orange is actual\n",
        "\n",
        "#Note to self: check to see that it is actually comparing predictions to TEST DATA\n",
        "    \n",
        "\n",
        "#Graphing\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Value\n",
            "Date                 \n",
            "1959-12-31  -2.436508\n",
            "1960-03-31  -1.574586\n",
            "1960-06-30  -0.298077\n",
            "1960-09-30   0.246575\n",
            "1960-12-31  -0.483516\n",
            "...               ...\n",
            "2017-12-31  -0.310452\n",
            "2018-03-31   0.278582\n",
            "2018-06-30  -0.951335\n",
            "2018-09-30  43.060628\n",
            "2018-12-31  -0.316263\n",
            "\n",
            "[237 rows x 1 columns]\n",
            "datasix\n",
            "data six\n",
            "                Value\n",
            "Date                 \n",
            "1960-03-31  -1.574586\n",
            "1960-06-30  -0.298077\n",
            "1960-09-30   0.246575\n",
            "1960-12-31  -0.483516\n",
            "1961-03-31   1.021277\n",
            "...               ...\n",
            "2017-12-31  -0.310452\n",
            "2018-03-31   0.278582\n",
            "2018-06-30  -0.951335\n",
            "2018-09-30  43.060628\n",
            "2018-12-31  -0.316263\n",
            "\n",
            "[236 rows x 1 columns]\n",
            "            S&P Composite  Dividend  ...  Inflation Rate  Misery Index\n",
            "1959-12-31       0.035232  0.011050  ...        0.253623      0.021802\n",
            "1960-03-31      -0.068405  0.060109  ...        0.000000      0.014225\n",
            "1960-06-30       0.040712  0.005155  ...       -0.005780     -0.001403\n",
            "1960-09-30      -0.042787  0.000000  ...       -0.406977     -0.084270\n",
            "1960-12-31       0.036307  0.000000  ...        0.333333      0.220859\n",
            "...                   ...       ...  ...             ...           ...\n",
            "2017-12-31       0.068797  0.015777  ...       -0.053812     -0.034215\n",
            "2018-03-31       0.014424  0.021868  ...        0.118483      0.040258\n",
            "2018-06-30       0.019084  0.019800  ...        0.216102      0.063467\n",
            "2018-09-30       0.053425  0.026476  ...       -0.205575     -0.129549\n",
            "2018-12-31      -0.115178  0.026939  ...       -0.162281     -0.028428\n",
            "\n",
            "[237 rows x 13 columns]\n",
            "[[ 5.79203  ]\n",
            " [ 1.34229  ]\n",
            " [-0.328963 ]\n",
            " [-0.391626 ]\n",
            " [ 1.90183  ]\n",
            " [ 5.79442  ]\n",
            " [ 6.4968   ]\n",
            " [ 7.8818   ]\n",
            " [ 5.05793  ]\n",
            " [ 4.51151  ]\n",
            " [ 3.86883  ]\n",
            " [ 4.55533  ]\n",
            " [ 4.04211  ]\n",
            " [ 5.59246  ]\n",
            " [ 4.79488  ]\n",
            " [ 4.57696  ]\n",
            " [ 5.70386  ]\n",
            " [ 5.77085  ]\n",
            " [ 5.12072  ]\n",
            " [ 5.27936  ]\n",
            " [ 7.19789  ]\n",
            " [ 5.98077  ]\n",
            " [ 6.13003  ]\n",
            " [ 7.81088  ]\n",
            " [ 6.82991  ]\n",
            " [ 3.65192  ]\n",
            " [ 3.00844  ]\n",
            " [ 3.23455  ]\n",
            " [ 2.32443  ]\n",
            " [ 2.77207  ]\n",
            " [ 3.89892  ]\n",
            " [ 4.65521  ]\n",
            " [ 5.44585  ]\n",
            " [ 5.30513  ]\n",
            " [ 4.80952  ]\n",
            " [ 3.57479  ]\n",
            " [ 3.19119  ]\n",
            " [ 2.53623  ]\n",
            " [ 1.74349  ]\n",
            " [-0.266546 ]\n",
            " [-1.43488  ]\n",
            " [ 0.372749 ]\n",
            " [ 1.55772  ]\n",
            " [ 0.157684 ]\n",
            " [ 4.89411  ]\n",
            " [ 3.27026  ]\n",
            " [ 3.43351  ]\n",
            " [ 5.2628   ]\n",
            " [ 5.73162  ]\n",
            " [ 5.09541  ]\n",
            " [ 6.70557  ]\n",
            " [ 8.86805  ]\n",
            " [ 6.14461  ]\n",
            " [ 2.39464  ]\n",
            " [ 2.063    ]\n",
            " [ 2.15205  ]\n",
            " [-1.45832  ]\n",
            " [-1.92259  ]\n",
            " [-2.14581  ]\n",
            " [-3.89255  ]\n",
            " [-2.30409  ]\n",
            " [ 3.09246  ]\n",
            " [ 6.39202  ]\n",
            " [ 5.15312  ]\n",
            " [ 6.19155  ]\n",
            " [ 3.23315  ]\n",
            " [ 3.03576  ]\n",
            " [ 2.45687  ]\n",
            " [ 4.82759  ]\n",
            " [ 7.6408   ]\n",
            " [ 5.97181  ]\n",
            " [ 3.49203  ]\n",
            " [ 4.79085  ]\n",
            " [ 8.17563  ]\n",
            " [ 4.10268  ]\n",
            " [ 3.26017  ]\n",
            " [ 2.49731  ]\n",
            " [-0.340506 ]\n",
            " [ 0.337329 ]\n",
            " [ 0.871606 ]\n",
            " [-1.30615  ]\n",
            " [-3.098    ]\n",
            " [ 3.06715  ]\n",
            " [ 6.80654  ]\n",
            " [ 2.58093  ]\n",
            " [ 1.64311  ]\n",
            " [ 2.64545  ]\n",
            " [-2.49919  ]\n",
            " [-1.73221  ]\n",
            " [ 0.242102 ]\n",
            " [-0.664651 ]\n",
            " [ 0.0786178]\n",
            " [ 4.1591   ]\n",
            " [ 6.04926  ]\n",
            " [ 6.15629  ]\n",
            " [ 8.4147   ]\n",
            " [ 9.23808  ]\n",
            " [ 6.32872  ]\n",
            " [ 4.50995  ]\n",
            " [ 4.24965  ]\n",
            " [ 3.5434   ]\n",
            " [ 3.41461  ]\n",
            " [ 3.95261  ]\n",
            " [ 2.81839  ]\n",
            " [ 3.20738  ]\n",
            " [ 1.97781  ]\n",
            " [ 2.42752  ]\n",
            " [ 3.23469  ]\n",
            " [ 4.25261  ]\n",
            " [ 5.62971  ]\n",
            " [ 5.77631  ]\n",
            " [ 4.88257  ]\n",
            " [ 4.855    ]\n",
            " [ 4.27421  ]\n",
            " [ 4.08692  ]\n",
            " [ 4.51391  ]\n",
            " [ 2.49301  ]\n",
            " [ 0.418549 ]\n",
            " [ 1.26007  ]\n",
            " [ 1.35855  ]\n",
            " [ 2.72697  ]\n",
            " [ 1.76043  ]\n",
            " [-0.322732 ]\n",
            " [-1.07429  ]\n",
            " [-0.0692295]\n",
            " [ 0.974605 ]\n",
            " [ 1.31889  ]\n",
            " [ 2.3353   ]\n",
            " [ 5.22727  ]\n",
            " [ 3.47557  ]\n",
            " [ 2.03133  ]\n",
            " [ 2.58641  ]\n",
            " [ 0.45102  ]\n",
            " [ 3.46622  ]\n",
            " [ 2.49934  ]\n",
            " [ 5.73686  ]\n",
            " [ 3.52112  ]\n",
            " [ 4.95061  ]\n",
            " [ 4.09487  ]\n",
            " [ 3.88778  ]\n",
            " [ 2.26596  ]\n",
            " [ 2.76656  ]\n",
            " [ 4.07994  ]\n",
            " [ 3.2402   ]\n",
            " [ 4.5535   ]\n",
            " [ 4.96334  ]\n",
            " [ 4.08749  ]\n",
            " [ 4.5329   ]\n",
            " [ 4.81146  ]\n",
            " [ 5.0082   ]\n",
            " [ 5.64033  ]\n",
            " [ 5.21966  ]\n",
            " [ 4.48614  ]\n",
            " [ 5.33569  ]\n",
            " [ 5.26808  ]\n",
            " [ 4.02887  ]\n",
            " [ 4.56124  ]\n",
            " [ 3.35166  ]\n",
            " [ 4.18453  ]\n",
            " [ 5.44795  ]\n",
            " [ 7.02761  ]\n",
            " [ 2.74501  ]\n",
            " [ 2.74998  ]\n",
            " [ 0.407766 ]\n",
            " [ 3.19998  ]\n",
            " [-0.100841 ]\n",
            " [-0.927735 ]\n",
            " [-0.599602 ]\n",
            " [ 3.09227  ]\n",
            " [ 2.86451  ]\n",
            " [ 1.20552  ]\n",
            " [ 2.36795  ]\n",
            " [ 1.63761  ]\n",
            " [ 3.44626  ]\n",
            " [ 3.63754  ]\n",
            " [ 3.09285  ]\n",
            " [ 3.56996  ]\n",
            " [ 4.52768  ]\n",
            " [ 4.25835  ]\n",
            " [ 2.87328  ]\n",
            " [ 3.53497  ]\n",
            " [ 3.45902  ]\n",
            " [ 3.39299  ]\n",
            " [ 5.61161  ]\n",
            " [ 5.20711  ]\n",
            " [ 1.78291  ]\n",
            " [ 1.92163  ]\n",
            " [ 1.19981  ]\n",
            " [ 0.412618 ]\n",
            " [ 0.569938 ]\n",
            " [-1.73     ]\n",
            " [-0.898518 ]\n",
            " [ 0.0769245]\n",
            " [-1.2496   ]\n",
            " [-2.8226   ]\n",
            " [-6.14489  ]\n",
            " [-4.68704  ]\n",
            " [ 0.7185   ]\n",
            " [ 2.08095  ]\n",
            " [ 3.86987  ]\n",
            " [ 2.86261  ]\n",
            " [ 4.13249  ]\n",
            " [ 4.80083  ]\n",
            " [ 1.55757  ]\n",
            " [ 1.17476  ]\n",
            " [ 2.29793  ]\n",
            " [ 2.44344  ]\n",
            " [ 3.59489  ]\n",
            " [ 5.94664  ]\n",
            " [ 1.95291  ]\n",
            " [-0.455773 ]\n",
            " [ 2.70585  ]\n",
            " [ 1.01155  ]\n",
            " [ 2.12825  ]\n",
            " [ 1.12048  ]\n",
            " [ 2.2386   ]\n",
            " [ 3.53684  ]\n",
            " [ 4.44024  ]\n",
            " [ 4.08105  ]\n",
            " [ 3.17901  ]\n",
            " [ 2.71998  ]\n",
            " [ 1.42263  ]\n",
            " [ 1.02912  ]\n",
            " [ 0.716699 ]\n",
            " [ 1.21727  ]\n",
            " [ 0.0507613]\n",
            " [ 1.69025  ]\n",
            " [ 2.33425  ]\n",
            " [ 3.1253   ]\n",
            " [ 2.64049  ]\n",
            " [ 1.67384  ]\n",
            " [ 1.8482   ]\n",
            " [ 3.24842  ]\n",
            " [ 1.80685  ]\n",
            " [ 3.43576  ]\n",
            " [ 2.30958  ]\n",
            " [ 2.82159  ]]\n",
            "[[ 5.79203  ]\n",
            " [ 1.34229  ]\n",
            " [-0.328963 ]\n",
            " [-0.391626 ]\n",
            " [ 1.90183  ]\n",
            " [ 5.79442  ]\n",
            " [ 6.4968   ]\n",
            " [ 7.8818   ]\n",
            " [ 5.05793  ]\n",
            " [ 4.51151  ]\n",
            " [ 3.86883  ]\n",
            " [ 4.55533  ]\n",
            " [ 4.04211  ]\n",
            " [ 5.59246  ]\n",
            " [ 4.79488  ]\n",
            " [ 4.57696  ]\n",
            " [ 5.70386  ]\n",
            " [ 5.77085  ]\n",
            " [ 5.12072  ]\n",
            " [ 5.27936  ]\n",
            " [ 7.19789  ]\n",
            " [ 5.98077  ]\n",
            " [ 6.13003  ]\n",
            " [ 7.81088  ]\n",
            " [ 6.82991  ]\n",
            " [ 3.65192  ]\n",
            " [ 3.00844  ]\n",
            " [ 3.23455  ]\n",
            " [ 2.32443  ]\n",
            " [ 2.77207  ]\n",
            " [ 3.89892  ]\n",
            " [ 4.65521  ]\n",
            " [ 5.44585  ]\n",
            " [ 5.30513  ]\n",
            " [ 4.80952  ]\n",
            " [ 3.57479  ]\n",
            " [ 3.19119  ]\n",
            " [ 2.53623  ]\n",
            " [ 1.74349  ]\n",
            " [-0.266546 ]\n",
            " [-1.43488  ]\n",
            " [ 0.372749 ]\n",
            " [ 1.55772  ]\n",
            " [ 0.157684 ]\n",
            " [ 4.89411  ]\n",
            " [ 3.27026  ]\n",
            " [ 3.43351  ]\n",
            " [ 5.2628   ]\n",
            " [ 5.73162  ]\n",
            " [ 5.09541  ]\n",
            " [ 6.70557  ]\n",
            " [ 8.86805  ]\n",
            " [ 6.14461  ]\n",
            " [ 2.39464  ]\n",
            " [ 2.063    ]\n",
            " [ 2.15205  ]\n",
            " [-1.45832  ]\n",
            " [-1.92259  ]\n",
            " [-2.14581  ]\n",
            " [-3.89255  ]\n",
            " [-2.30409  ]\n",
            " [ 3.09246  ]\n",
            " [ 6.39202  ]\n",
            " [ 5.15312  ]\n",
            " [ 6.19155  ]\n",
            " [ 3.23315  ]\n",
            " [ 3.03576  ]\n",
            " [ 2.45687  ]\n",
            " [ 4.82759  ]\n",
            " [ 7.6408   ]\n",
            " [ 5.97181  ]\n",
            " [ 3.49203  ]\n",
            " [ 4.79085  ]\n",
            " [ 8.17563  ]\n",
            " [ 4.10268  ]\n",
            " [ 3.26017  ]\n",
            " [ 2.49731  ]\n",
            " [-0.340506 ]\n",
            " [ 0.337329 ]\n",
            " [ 0.871606 ]\n",
            " [-1.30615  ]\n",
            " [-3.098    ]\n",
            " [ 3.06715  ]\n",
            " [ 6.80654  ]\n",
            " [ 2.58093  ]\n",
            " [ 1.64311  ]\n",
            " [ 2.64545  ]\n",
            " [-2.49919  ]\n",
            " [-1.73221  ]\n",
            " [ 0.242102 ]\n",
            " [-0.664651 ]\n",
            " [ 0.0786178]\n",
            " [ 4.1591   ]\n",
            " [ 6.04926  ]\n",
            " [ 6.15629  ]\n",
            " [ 8.4147   ]\n",
            " [ 9.23808  ]\n",
            " [ 6.32872  ]\n",
            " [ 4.50995  ]\n",
            " [ 4.24965  ]\n",
            " [ 3.5434   ]\n",
            " [ 3.41461  ]\n",
            " [ 3.95261  ]\n",
            " [ 2.81839  ]\n",
            " [ 3.20738  ]\n",
            " [ 1.97781  ]\n",
            " [ 2.42752  ]\n",
            " [ 3.23469  ]\n",
            " [ 4.25261  ]\n",
            " [ 5.62971  ]\n",
            " [ 5.77631  ]\n",
            " [ 4.88257  ]\n",
            " [ 4.855    ]\n",
            " [ 4.27421  ]\n",
            " [ 4.08692  ]\n",
            " [ 4.51391  ]\n",
            " [ 2.49301  ]\n",
            " [ 0.418549 ]\n",
            " [ 1.26007  ]\n",
            " [ 1.35855  ]\n",
            " [ 2.72697  ]\n",
            " [ 1.76043  ]\n",
            " [-0.322732 ]\n",
            " [-1.07429  ]\n",
            " [-0.0692295]\n",
            " [ 0.974605 ]\n",
            " [ 1.31889  ]\n",
            " [ 2.3353   ]\n",
            " [ 5.22727  ]\n",
            " [ 3.47557  ]\n",
            " [ 2.03133  ]\n",
            " [ 2.58641  ]\n",
            " [ 0.45102  ]\n",
            " [ 3.46622  ]\n",
            " [ 2.49934  ]\n",
            " [ 5.73686  ]\n",
            " [ 3.52112  ]\n",
            " [ 4.95061  ]\n",
            " [ 4.09487  ]\n",
            " [ 3.88778  ]\n",
            " [ 2.26596  ]\n",
            " [ 2.76656  ]\n",
            " [ 4.07994  ]\n",
            " [ 3.2402   ]\n",
            " [ 4.5535   ]\n",
            " [ 4.96334  ]\n",
            " [ 4.08749  ]\n",
            " [ 4.5329   ]\n",
            " [ 4.81146  ]\n",
            " [ 5.0082   ]\n",
            " [ 5.64033  ]\n",
            " [ 5.21966  ]\n",
            " [ 4.48614  ]\n",
            " [ 5.33569  ]\n",
            " [ 5.26808  ]\n",
            " [ 4.02887  ]\n",
            " [ 4.56124  ]\n",
            " [ 3.35166  ]\n",
            " [ 4.18453  ]\n",
            " [ 5.44795  ]\n",
            " [ 7.02761  ]\n",
            " [ 2.74501  ]\n",
            " [ 2.74998  ]\n",
            " [ 0.407766 ]\n",
            " [ 3.19998  ]\n",
            " [-0.100841 ]\n",
            " [-0.927735 ]\n",
            " [-0.599602 ]\n",
            " [ 3.09227  ]\n",
            " [ 2.86451  ]\n",
            " [ 1.20552  ]\n",
            " [ 2.36795  ]\n",
            " [ 1.63761  ]\n",
            " [ 3.44626  ]\n",
            " [ 3.63754  ]\n",
            " [ 3.09285  ]\n",
            " [ 3.56996  ]\n",
            " [ 4.52768  ]\n",
            " [ 4.25835  ]\n",
            " [ 2.87328  ]\n",
            " [ 3.53497  ]\n",
            " [ 3.45902  ]\n",
            " [ 3.39299  ]\n",
            " [ 5.61161  ]\n",
            " [ 5.20711  ]\n",
            " [ 1.78291  ]\n",
            " [ 1.92163  ]\n",
            " [ 1.19981  ]\n",
            " [ 0.412618 ]\n",
            " [ 0.569938 ]\n",
            " [-1.73     ]\n",
            " [-0.898518 ]\n",
            " [ 0.0769245]\n",
            " [-1.2496   ]\n",
            " [-2.8226   ]\n",
            " [-6.14489  ]\n",
            " [-4.68704  ]\n",
            " [ 0.7185   ]\n",
            " [ 2.08095  ]\n",
            " [ 3.86987  ]\n",
            " [ 2.86261  ]\n",
            " [ 4.13249  ]\n",
            " [ 4.80083  ]\n",
            " [ 1.55757  ]\n",
            " [ 1.17476  ]\n",
            " [ 2.29793  ]\n",
            " [ 2.44344  ]\n",
            " [ 3.59489  ]\n",
            " [ 5.94664  ]\n",
            " [ 1.95291  ]\n",
            " [-0.455773 ]\n",
            " [ 2.70585  ]\n",
            " [ 1.01155  ]\n",
            " [ 2.12825  ]\n",
            " [ 1.12048  ]\n",
            " [ 2.2386   ]\n",
            " [ 3.53684  ]\n",
            " [ 4.44024  ]\n",
            " [ 4.08105  ]\n",
            " [ 3.17901  ]\n",
            " [ 2.71998  ]\n",
            " [ 1.42263  ]\n",
            " [ 1.02912  ]\n",
            " [ 0.716699 ]\n",
            " [ 1.21727  ]\n",
            " [ 0.0507613]\n",
            " [ 1.69025  ]\n",
            " [ 2.33425  ]\n",
            " [ 3.1253   ]\n",
            " [ 2.64049  ]\n",
            " [ 1.67384  ]\n",
            " [ 1.8482   ]\n",
            " [ 3.24842  ]\n",
            " [ 1.80685  ]\n",
            " [ 3.43576  ]\n",
            " [ 2.30958  ]\n",
            " [ 2.82159  ]]\n",
            "shape of x\n",
            "(237, 13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ninputs = tf.keras.Input(shape=(1,13))\\nfirst  = tf.keras.layers.LSTM(10,activation='relu',kernel_initializer='ones', use_bias=False)(inputs)\\n\\nu = tf.keras.layers.Dense(250,activation='relu',kernel_initializer='ones',use_bias=False)(first)\\nb = tf.keras.layers.BatchNormalization()(u)\\nu = tf.keras.layers.Dense(500, activation='relu',kernel_initializer='ones',use_bias=False)(b)\\nu = tf.keras.layers.Dense(10, activation='relu')(u)\\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(u)\\nModel = tf.keras.Model(inputs=inputs, outputs=outputs)\\n\\n#Adadelta\\nModel.compile(optimizer='Adagrad',\\n        loss='MSLE',\\n        metrics=['accuracy'])\\n\\t\\nModel.fit(X,Y,epochs=8, verbose=1,validation_split=0.2)#, callbacks=[early_stop])\\n\\nPrediction = Model.predict(X[189::])\\nprint(Prediction)\\nprint(Prediction.shape)\\nu = 219\\nModel.summary()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEZCAYAAABICyhRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTZJRMEbzRcd",
        "colab_type": "text"
      },
      "source": [
        "Hypotheses which underly this algorithm\n",
        "\n",
        "\n",
        "1. The data exhibits long term dependence. This is in part due to the fact that we believe components of the S&p 500 input variable exhibits long term dependence. Note: this hypothesis has been confirmed.\n",
        "\n",
        "2. IF hypothesis 1 is true the final posterior distribution does NOT follow a Standard normal distribution\n",
        "\n",
        "3. THe input variables DO NOT follow a Independent standard normal distribution but are correlated to one another\n",
        "\n",
        "what we need is a kernel which takes into account the fact that the data is time series (in other words we need a autocovariance function) and we need that auto covariance function to have long term dependence built in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z17xQAl08rDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hearst Exponent\n",
        "H, c, data = compute_Hc(y, kind='change', simplified=True)\n",
        "print(a.trend)\n",
        "print(np.abs(H))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBA3RlkAt9tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#labels = quandl.get(\"FRBP/GDPPLUS_042619\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", collapse=\"quarterly\")#quandl.get(\"FRBP/GDPPLUS_042619\", authtoken=\"DNMZo2iRzVENxpxqHBKF\")\n",
        "#labels=Scalar.fit_transform(labels)\n",
        "#b = decompose(labels)\n",
        "\n",
        "X = datafour[:,None]\n",
        "x = X\n",
        "#y = b\n",
        "#Y = a.trend\n",
        "#y = Y\n",
        "\n",
        "num_inducing_points = 45\n",
        "#custom time series kernel  integrate.quad(lambda x: special.jv(2.5,x), -π, π)e^(ih*lambda)*f(lambda)d(lambda)\n",
        "#default RBF kernel k(x, y) = amplitude**2 * exp(-||x - y||**2 / (2 * length_scale**2))\n",
        "\"\"\"\n",
        "  def kernel(self):\n",
        "    return 1/2*np.absolute(t)**(2H)+np.absolute(s)**(2H) - (np.absolute(t)-np.absolute(s))**2H \n",
        "    t = x\n",
        "    s = y\n",
        "    Calculating the average value, Xm, of the X1..Xn series\n",
        "    Calculating the standard series deviation, S\n",
        "    Normalization of the series by deducting the average value, Zr (where r=1..n), from each value\n",
        "    Creating a cumulative time series Y1=Z1+Zr, where r=2..n\n",
        "    Calculating the magnitude of the cumulative time series R=max(Y1..Yn)-min(Y1..Yn)\n",
        "    Dividing the magnitude of the cumulative time series by the standard deviation (S).\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#Time series gaussian kernel with long term dependence: E[BSubSccript(H)(H)BsubscriptH(s)] = 1/2|t|^2H-|t-s|&2H where H is the hearst exponent If we assume there is a long term dependence that means 1 > H > 1/2 \n",
        "\n",
        "x = x.astype(np.float32)#tf.dtypes.cast(x, tf.int32) #\n",
        "#x = tf.cast(x, tf.float32)\n",
        "#x = tensor_util.convert_nonref_to_tensor(x, dtype=x.dtype)\n",
        "\n",
        "class RBFKernelFn(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(RBFKernelFn, self).__init__(**kwargs)\n",
        "    dtype = kwargs.get('dtype', None)\n",
        "\n",
        "    self._amplitude = self.add_variable(\n",
        "            initializer=tf.constant_initializer(0),\n",
        "            dtype=dtype,\n",
        "            name='amplitude')\n",
        "    \n",
        "    self._length_scale = self.add_variable(\n",
        "            initializer=tf.constant_initializer(0),\n",
        "            dtype=dtype,\n",
        "            name='length_scale')\n",
        "  \n",
        "  def call(self, x):\n",
        "    # Never called -- this is just a layer so it can hold variables\n",
        "    # in a way Keras understands.\n",
        "    #print(dtype)\n",
        "    return x\n",
        "\n",
        "  @property\n",
        "  def kernel(self):\n",
        "\n",
        "    \n",
        "    return tfp.positive_semidefinite_kernels.ExponentiatedQuadratic(\n",
        "      amplitude=tf.nn.softplus(0.1 * self._amplitude),\n",
        "      length_scale=tf.nn.softplus(5. * self._length_scale)\n",
        "    )\n",
        "    \n",
        "     #x1 = x[0]\n",
        "    #x2 = x[1]\n",
        "\n",
        "    #return tf.convert_to_tensor(1/2*(np.absolute(x1))**(2*H)+(np.absolute(x2))**(2*H) - (np.absolute(x1)-np.absolute(x2))**2*H)#tf.as_dtype(1/2*(np.absolute(x))**(2*H)+(np.absolute(y))**(2*H) - (np.absolute(x)-np.absolute(y))**2*H)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUO6_atu9SsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is in reality a fractional brownian motion kernel\n",
        "\"\"\"\n",
        "class Brownian(gpflow.kernels.Kernel):\n",
        "    def __init__(self):\n",
        "        super().__init__(active_dims=[0])\n",
        "        #self.variance = gpflow.Param(1.0, transform=gpflow.transforms.positive)\n",
        "\n",
        "    #@gpflow.params_as_tensors\n",
        "    def K(self, X, X2=None):\n",
        "        if X2 is None:\n",
        "            X2 = X\n",
        "        return 1/2*(np.absolute(X))**(2*H)+(np.absolute(X2))**(2*H) - (np.absolute(X))-(np.absolute(X2))**(2*H)\n",
        "    def K_diag(self, X, presliced=None):\n",
        "        return tp.astype(self.variance * tf.reshape(X, (-1,)),tf.int64 )  \n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gty_0ggSjbhm",
        "colab_type": "text"
      },
      "source": [
        "#custom gaussian time series kernel logic\n",
        "## #the autocovariance function is ysubscript(f)(h) = integral pi, -p^ih*lambda*f(lambda)*d(lambda) is the function if P^nSubscript(f) is the distribution of X1, ... Xn for a stationary mean-zero gaussian time series (Xsubscript(t):t strange symbol which looks like E Z)\n",
        "\n",
        "---\n",
        "This is the final thing I have to implement for phase 3 to be complete. Unfortunately I will need to wait until gpflow is updated before I can make that final update\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYBJkztebntB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "62b37240-6df4-438c-e071-3f63793a304c"
      },
      "source": [
        "\n",
        "#bijector=tfb.BatchNorm()\n",
        "#kernel = tfk.\n",
        "    #tf.keras.layers.BatchNormalization(),\n",
        "    #tf.keras.layers.Dense(250, activation='sigmoid',kernel_initializer='ones', use_bias=False),\n",
        "#datafour = x\n",
        "#trainingtarget = y\n",
        "#x = datafour\n",
        "#y = trainingtarget\n",
        "x_tst = x[189::]\n",
        "x_range = 237\n",
        "num_distributions_over_Functions = 1\n",
        "\n",
        "#kernel = Brownian #tfp.positive_semidefinite_kernels.ExponentiatedQuadratic#MaternOneHalf()\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(1,13), dtype=x.dtype),\n",
        "    tf.keras.layers.LSTM(25,kernel_initializer='ones', dtype = x.dtype, use_bias=False),\n",
        "    #tf.keras.layers.InputLayer(input_shape=(10),dtype=x.dtype),#put a 1 before the 9 later\n",
        "    tf.keras.layers.Dense(50,kernel_initializer='ones', use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(100,kernel_initializer='ones', use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(175,kernel_initializer='ones',use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(250,kernel_initializer='ones',use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\n",
        "    #goal is to eventually replace the first dense layer with an LSTM layer \n",
        "    #tf.keras.layers.LSTM\n",
        "    #tf.keras.layers.TimeDistributed(Dense(vocabulary)))\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(100,kernel_initializer='ones',use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(50,kernel_initializer='ones',use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(25, kernel_initializer='ones',use_bias=False,),\n",
        "    tfp.layers.VariationalGaussianProcess(\n",
        "        num_inducing_points=num_inducing_points,\n",
        "        kernel_provider=RBFKernelFn(dtype=x.dtype),\n",
        "        inducing_index_points_initializer=tf.constant_initializer(\n",
        "            np.linspace(0,x_range, num=1125,#num_inducing_points,\n",
        "                        dtype=x.dtype)[..., np.newaxis]),\n",
        "        unconstrained_observation_noise_variance_initializer=(\n",
        "            tf.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),\n",
        "        event_shape=[num_distributions_over_Functions],jitter=1e-06\n",
        "\n",
        "    )\n",
        "    #in unconstrained thing replace astype with tf.dtype thing.\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "#AttributeError: 'numpy.dtype' object has no attribute 'as_numpy_dtype' when trying to implement custom kernel\n",
        "\n",
        "#TypeError: linspace() missing 1 required positional argument: 'stop' This is the error to be resolved before I can add the inducing_indexpoints_initializer"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-caee73e0f491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ones'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     tfp.layers.VariationalGaussianProcess(\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mnum_inducing_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_inducing_points\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mkernel_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRBFKernelFn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tfp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDgNdpqNWiHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sherpa.algorithms.bayesian_optimization as bayesian_optimization\n",
        "parameters = [sherpa.Continuous('lrinit', [0.01, 0.011], 'log')]\n",
        "              #sherpa.Continuous('lrdecay', [1e-2, 1e-7], 'log')]\n",
        "alg = bayesian_optimization.GPyOpt(max_num_trials=50)#sherpa.algorithms.GPyOpt('GP', num_initial_data_points='infer',initial_data_points=[0.1,0.11,0.12], acquisition_type='MPI',verbosity=True) \n",
        "study = sherpa.Study(parameters=parameters,\n",
        "                     algorithm=alg,\n",
        "                     lower_is_better=False)\n",
        "\n",
        "batch_size =19\n",
        "loss = lambda y, rv_y: rv_y.variational_loss(\n",
        "    y, kl_weight=np.array(batch_size, x.dtype) / x.shape[0])\n",
        "num_iterations = 5\n",
        "epochs = 20\n",
        "for trial in study:\n",
        "\n",
        "  \n",
        "\n",
        "  lr = trial.parameters['lrinit']\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.Input(shape=(1,13), dtype=x.dtype),\n",
        "      tf.keras.layers.LSTM(25,kernel_initializer='ones', dtype = x.dtype, use_bias=False),\n",
        "      #tf.keras.layers.InputLayer(input_shape=(10),dtype=x.dtype),#put a 1 before the 9 later\n",
        "      tf.keras.layers.Dense(50,kernel_initializer='ones', use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(100,kernel_initializer='ones', use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(175,kernel_initializer='ones',use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(250,kernel_initializer='ones',use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\n",
        "      #goal is to eventually replace the first dense layer with an LSTM layer \n",
        "      #tf.keras.layers.LSTM\n",
        "      #tf.keras.layers.TimeDistributed(Dense(vocabulary)))\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(100,kernel_initializer='ones',use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(50,kernel_initializer='ones',use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(25, kernel_initializer='ones',use_bias=False,),\n",
        "      tfp.layers.VariationalGaussianProcess(\n",
        "          num_inducing_points=num_inducing_points,\n",
        "          kernel_provider=RBFKernelFn(dtype=x.dtype),\n",
        "          inducing_index_points_initializer=tf.constant_initializer(\n",
        "              np.linspace(0,x_range, num=1125,#num_inducing_points,\n",
        "                          dtype=x.dtype)[..., np.newaxis]),\n",
        "          unconstrained_observation_noise_variance_initializer=(\n",
        "              tf.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),\n",
        "          event_shape=[num_distributions_over_Functions],jitter=1e-06\n",
        "\n",
        "      )\n",
        "      #in unconstrained thing replace astype with tf.dtype thing.\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
        "  model.compile(optimizer=optimizer, loss=loss)\n",
        "  for i in range(epochs):\n",
        "\n",
        "    model.fit(x, y,epochs=epochs, verbose=True,validation_split=0.2)\n",
        "    loss= model.evaluate(x[189::],y[189::])\n",
        "    study.add_observation(trial=trial,iteration=i,objective=loss,context={'loss':loss})\n",
        "  #training_error = model.fit(epochs=1)\n",
        "\n",
        " \n",
        "  study.finalize(trial=trial)\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5gJ1T5NaNwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do inference.\n",
        "#Note: look into BATCHES and make sure they are fed in in order second potential problem to explore: It is possible I already looked into this possibility in phase 1 but make sure that it isnt using input data from the same day to create its predictions\n",
        "batch_size =19\n",
        "#use a different loss other than the variational gaussian loss\n",
        "\n",
        "loss = lambda y, rv_y: rv_y.variational_loss(\n",
        "    y, kl_weight=np.array(batch_size, x.dtype) / x.shape[0])\n",
        "\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.011), loss=loss)#tf.optimizers.Adam(learning_rate=0.01)\n",
        "model.fit(x, y,epochs=290, verbose=True,validation_split=0.2)\n",
        "#model.predict(x)\n",
        "#adjust it to 240 epochs if using pca first\n",
        "#prediction = model.predict(x_tst)\n",
        "# Make predictions. \n",
        "#yhats = [model(x_tst) for _ in range(100)]\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKo0b37Thkte",
        "colab_type": "text"
      },
      "source": [
        "what this algorithm is a joint distribution describing the relationship between the input variables and economic growth. this joint distribution is actually a distribution over functions between input and output.\n",
        "  We then use these to determine a predictive distribution and make point predictions Although this can be used to make a overall predictive distribution in reality this is far more useful for making a distribution over every point in the function. In other words a contnuous set of changing liklihoods for our point predictions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl7Qz9nNxDwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat = model(x_tst)#Note that this is a distribution not a tensor\n",
        "num_samples = 6 #note: num_samples refers to how many generated future evolutions of economic growth you want it to generate over the specified period of time\n",
        "Models = []\n",
        "#probability = \n",
        "average = []\n",
        "\"\"\"\n",
        "note: the below code works for turning from a tensorflow distribution back to a tensor\n",
        "a = tf.compat.v1.convert_to_tensor(\n",
        "    yhat,\n",
        "    dtype=None,\n",
        "    name=None,\n",
        "    preferred_dtype=None,\n",
        "    dtype_hint=None\n",
        ")\n",
        "\"\"\"\n",
        "#plt.plot(a)\n",
        "#plt.plot(yhats)#Note: yhat is supposedly a distribution and not a tensor explore the possibility that it is outputting probabilities and not tensors\n",
        "for i in range(num_samples):\n",
        "  #plt.plot(yhat)\n",
        "  sample_ = yhat.sample().numpy()\n",
        "  #Model = sample\n",
        "  #sample_[..., 0].T,\n",
        "  #print(model.summary)\n",
        "  mean = yhat.mean().numpy()\n",
        "  #print(mean)\n",
        "  #variance = yhat.variance().numpy()\n",
        "  #std = yhat.stddev().numpy()\n",
        "  #print(yhat.sample().numpy)\n",
        " \n",
        "  print(len(r[0]))\n",
        "  plt.plot(sample_[..., 0].T,\n",
        "           'r',\n",
        "           linewidth=0.2,\n",
        "           label='ensemble means' if i == 0 else None);\n",
        "           \n",
        "\"\"\"\n",
        "(variational_loss,\n",
        "variational_distributions) = tfp.sts.build_factored_variational_loss(\n",
        "model=model, observed_time_series=observed_time_series,\n",
        "init_batch_shape=[10])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#a = yhat.sample_[..., 0].T\n",
        "\n",
        "print(\"Predictions\")\n",
        "#print(predictions)\n",
        "#plt.plot(predictions)\n",
        "\n",
        "#forecast_dist = tfp.sts.forecast(model, observed_time_series,parameter_samples=sample_,num_steps_forecast=5)\n",
        "print(sample_)\n",
        "\n",
        "r = []\n",
        "\n",
        "#r = pd.DataFrame(r)\n",
        "\n",
        "for i in range(num_samples):\n",
        "  sample_ = yhat.sample().numpy()\n",
        "  #sns.kdeplot(sample_[..., 0].T)\n",
        "  e = sample_[..., 0].T\n",
        "  print(\"asdf\")\n",
        "  #print(len(e))\n",
        "  r.append(e)\n",
        "  print(len(r[0]))\n",
        "rfinal = (r[0]+r[1]+r[2]+r[3]+r[4]+r[5])/6\n",
        "plt.plot(rfinal)\n",
        "u1 = r[0]\n",
        "u2 = r[1]\n",
        "u3 = r[2]\n",
        "u4 = r[3]\n",
        "u5 = r[4]\n",
        "u6 = r[5]\n",
        "ufinal = ((u1 - rfinal) + (u2-rfinal) + (u4-rfinal) + (u5 - rfinal) + (u6-rfinal))/6\n",
        "from shapely.geometry import LineString\n",
        "\n",
        "#note: use shapely to fine intersection points\n",
        "#first_line = LineString(np.column_stack((x, f)))\n",
        "#second_line = LineString(np.column_stack((x, g)))\n",
        "#intersection = first_line.intersection(second_line)\n",
        "\n",
        "#this is for plotting the intersection points\n",
        "#plt.plot(*LineString(intersection).xy, 'o')\n",
        "\n",
        "#this is for getting the x and y of the intersection \n",
        "#x, y = LineString(intersection).xy\n",
        "\n",
        "print(ufinal)\n",
        "print(len(ufinal))#note ufinal len is 48\n",
        "\n",
        "\n",
        "plt.plot(y[189::]) \n",
        "print(\"average\")\n",
        "#st_dev = average.mean()#stddev()\n",
        "#average = mean#.mean()#.numpy()\n",
        "\n",
        "#plt.plot(sample_[20::])\n",
        "print(\"samples\")\n",
        "#print(sample_.T[20::])\n",
        "print(sample_)\n",
        "print(sample_.shape)\n",
        "#plt.plot(average[20::])\n",
        "#print(len(average))#Note that supposedly the average error rate for the Fed's current forcasting model called gdpNow is 0.56%\n",
        "#print(std)\n",
        "error = rfinal/y[189::] #if this was a standard normal distribution than 95% would be 0.252 and 0.968\n",
        "print(\"error\")\n",
        "print(error)\n",
        "print(\"average error\")\n",
        "print(np.sum(error)/len(error))\n",
        "#c = np.corrcoef(rfinal, y[:-189:])[0, 1]\n",
        "#print(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AHqEHmWLNQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import signal\n",
        "#average error rate is about 10%\n",
        "r = []\n",
        "\n",
        "#r = pd.DataFrame(r)\n",
        "\n",
        "for i in range(num_samples):\n",
        "  sample_ = yhat.sample().numpy()\n",
        "  sns.kdeplot(sample_[..., 0].T)\n",
        "  e = sample_[..., 0].T\n",
        "  r.append(e)\n",
        "\n",
        "  #Model = sample\n",
        "  #sample_[..., 0].T,\n",
        "  #print(model.summary)\n",
        "  #print(yhat.log_prob(y[189::]))\n",
        "  mean = yhat.mean().numpy()\n",
        "  #print(mean)\n",
        "  #variance = yhat.variance().numpy()\n",
        "  #std = yhat.stddev().numpy()\n",
        "  #print(yhat.sample().numpy)\n",
        "  #you have to figure out what the x and y axis are before you present\n",
        "  #it appears from this plot of economic growth has a very low hurst exponent\n",
        "print(r[0])\n",
        "print(r[1])\n",
        "rfinal = (r[0]+r[1]+r[2]+r[3]+r[4]+r[5])/6\n",
        "\n",
        "One = r[0]\n",
        "two = r[1]\n",
        "Three = r[2]\n",
        "four = r[3]\n",
        "five = r[4]\n",
        "\n",
        "pone = signal.fftconvolve(r[0],r[1],'same')\n",
        "ptwo = signal.fftconvolve(r[2],r[3],'same')\n",
        "pthree = signal.fftconvolve(r[4],r[5],'same')\n",
        "\n",
        "pfinal = signal.fftconvolve(pone,ptwo)\n",
        "pfinal = signal.fftconvolve(pfinal,pthree, 'same')\n",
        "\n",
        "\n",
        "\n",
        "  #yhat.prob()\n",
        "  #print(mean)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKfhuwuT27xC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.kdeplot(rfinal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc1pLECTwV9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfp.stats.stddev(\n",
        "    yhat,\n",
        "    sample_axis=0,\n",
        "    keepdims=False,\n",
        "    name=None\n",
        ")#yhat\n",
        "#observed_time_series = x_tst\n",
        "#co2_by_month_training_data = co2_by_month[:-num_forecast_steps] this has to be the format in which \n",
        "#between 0.16 and 0.18"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1LWVKD29Q4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "y = np.squeeze(y) \n",
        "sns.kdeplot(y[189::])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkYV_7NxAUnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSOzE67dYJZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#confidence  and credible intervals\n",
        "!pip install bayesint\n",
        "from bayesint import rel_risk\n",
        "for i in range(num_samples):\n",
        "  #print(\"Credible interval\")\n",
        "  #print(rel_risk(sample_[..., 0].T))\n",
        "  print(bs.bootstrap(sample_[..., 0].T, stat_func=bs_stats.mean))\n",
        "print(y[189::])\n",
        "#print(\"probability\")\n",
        "#print(yhat.log_prob([0,0.34]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXi4JUQRcqPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
        "x = np.squeeze(x)\n",
        "regr.fit(x[189::], y[189::])\n",
        "plt.plot(y[189::])\n",
        "#plt.plt(48)\n",
        "plt.plot(regr.predict(x[:48:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVVQeD7Hyiih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n",
        "#         Jake Vanderplas <vanderplas@astro.washington.edu>\n",
        "#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>s\n",
        "# License: BSD 3 clause\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "\n",
        "def f(x):\n",
        "    \"\"\"The function to predict.\"\"\"\n",
        "    return x * np.sin(x)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "#  First the noiseless case\n",
        "X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T\n",
        "\n",
        "# Observations\n",
        "y = f(X).ravel()\n",
        "\n",
        "# Mesh the input space for evaluations of the real function, the prediction and\n",
        "# its MSE\n",
        "x = np.atleast_2d(np.linspace(0, 10, 1000)).T\n",
        "\n",
        "# Instantiate a Gaussian Process model\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
        "\n",
        "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
        "gp.fit(X, y)\n",
        "\n",
        "# Make the prediction on the meshed x-axis (ask for MSE as well)\n",
        "y_pred, sigma = gp.predict(x, return_std=True)\n",
        "\n",
        "# Plot the function, the prediction and the 95% confidence interval based on\n",
        "# the MSE\n",
        "plt.figure()\n",
        "plt.plot(x, f(x), 'r:', label=r'$f(x) = x\\,\\sin(x)$')\n",
        "plt.plot(X, y, 'r.', markersize=10, label='Observations')\n",
        "plt.plot(x, y_pred, 'b-', label='Prediction')\n",
        "plt.fill(np.concatenate([x, x[::-1]]),\n",
        "         np.concatenate([y_pred - 1.9600 * sigma,\n",
        "                        (y_pred + 1.9600 * sigma)[::-1]]),\n",
        "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$f(x)$')\n",
        "plt.ylim(-10, 20)\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# now the noisy case\n",
        "X = np.linspace(0.1, 9.9, 20)\n",
        "X = np.atleast_2d(X).T\n",
        "\n",
        "# Observations and noise\n",
        "y = f(X).ravel()\n",
        "dy = 0.5 + 1.0 * np.random.random(y.shape)\n",
        "noise = np.random.normal(0, dy)\n",
        "y += noise\n",
        "\n",
        "# Instantiate a Gaussian Process model\n",
        "gp = GaussianProcessRegressor(kernel=kernel, alpha=dy ** 2,\n",
        "                              n_restarts_optimizer=10)\n",
        "\n",
        "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
        "gp.fit(X, y)\n",
        "\n",
        "# Make the prediction on the meshed x-axis (ask for MSE as well)\n",
        "y_pred, sigma = gp.predict(x, return_std=True)\n",
        "\n",
        "# Plot the function, the prediction and the 95% confidence interval based on\n",
        "# the MSE\n",
        "plt.figure()\n",
        "plt.plot(x, f(x), 'r:', label=r'$f(x) = x\\,\\sin(x)$')\n",
        "plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label='Observations')\n",
        "plt.plot(x, y_pred, 'b-', label='Prediction')\n",
        "\n",
        "plt.fill(np.concatenate([x, x[::-1]]),\n",
        "         np.concatenate([y_pred - 1.9600 * sigma,\n",
        "                        (y_pred + 1.9600 * sigma)[::-1]]),\n",
        "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
        "\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$f(x)$')\n",
        "plt.ylim(-10, 20)\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOwlBu4XftcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.kdeplot(regr.predict(x[:48:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVqahm0Osu54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_kernel(X, X2,H):\n",
        "    \"\"\"\n",
        "    We create a custom kernel:\n",
        "\n",
        "                 (2  0)\n",
        "    k(X, Y) = X  (    ) Y.T\n",
        "                 (0  1)\n",
        "    \"\"\"\n",
        "    H = 0.85\n",
        "    return 1/2*(np.absolute(X))**(2*H)+(np.absolute(X2))**(2*H) - (np.absolute(X))-(np.absolute(X2))**(2*H)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y1wb61Ab_oP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "#y = y.ravel()\n",
        "\n",
        "r, g = load_iris(return_X_y=True)\n",
        "print(r)\n",
        "print(g)\n",
        "\n",
        "#x = pd.DataFrame(x)\n",
        "#y = pd.DataFrame(y)\n",
        "X = np.squeeze(X)\n",
        "\n",
        "gpc = GaussianProcessRegressor(kernel=my_kernel,random_state=0).fit(X, trainingtarget.ravel())\n",
        "a = gpc.predict_proba(x[:2,:])\n",
        "print(\"gpc\")\n",
        "print(a)\n",
        "#gpc = GaussianProcessClassifier().fit(x,y)\n",
        "a = gpc.predict(x[220::])\n",
        "b = gpc.log_marginal_likelihood()\n",
        "#b = gpc.predict(x[220::])\n",
        "#score = gpr.score(x,y)\n",
        "#is the great score because training and test arent seperated?\n",
        "\n",
        "#print(b)\n",
        "#print(gpc.score(x[220::],y[220::]))\n",
        "#print(score)\n",
        "print(-1*gpc.log_marginal_likelihood())\n",
        "print(a)\n",
        "plt.plot(a)\n",
        "plt.plot(y[220::]) \n",
        "#model.fit(x, y,epochs=50, verbose=True,validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTR0JXrdNSLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#List of stuff left to do\n",
        "\n",
        "#1. average out all the various evolutions and see if you can create an additional composite line for economic growth. Bonus: see if you can do a weighted average so that for example stuff outside of the 95% confidence interval at that point is given less weight\n",
        "#2. Allow it to predict the future and allow it to give an 95% confidence interval for that one prediction. Also make sure that the various possible evolutions of economic growth and the composite are part of that\n",
        "#3.figure out a way to convert the first Dense layer to an LSTM layer\n",
        "#4. This is the easiest make it have 2 stages with an exponential decay for the second stage learning rate so the learning rate never becomes negative\n",
        "modelOne = []\n",
        "modelTwo = []\n",
        "modelThree = []\n",
        "modelFour = []\n",
        "modelFive = []\n",
        "modelSix = []\n",
        "\n",
        "print(len(sample_))\n",
        "    #suspected current loss function\n",
        "\"\"\"\n",
        "vgp.variational_loss(\n",
        "    observations=y_train_batch,\n",
        "    observation_index_points=x_train_batch,\n",
        "    kl_weight=float(batch_size) / float(num_training_points_))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzttMg_CrafG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gpc.predict_proba(X[:2,:])\n",
        "plt.plot(x_tst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4i6pKX6flyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#second stage of Janus Prediction engine\n",
        "time_series = np.linspace(1960.25,2016.25,224)\n",
        "observed_time_series = tfp.sts.MaskedTimeSeries(\n",
        "  time_series=time_series, is_missing=None\n",
        "  )\n",
        "\"\"\"\n",
        "local_level_model = LocalLevelStateSpaceModel(\n",
        "    num_timesteps=50,\n",
        "    level_scale=0.5,\n",
        "    initial_state_prior=tfd.MultivariateNormalDiag(scale_diag=[1.]))\n",
        "\"\"\"\n",
        "\n",
        "ndims = 2\n",
        "step_std = 1.0\n",
        "noise_std = 5.0\n",
        "\n",
        "\n",
        "predictions = tfp.sts.forecast(observed_time_series,parameter_samples=sample_,observed_time_series=observed_time_series,num_steps_forecast=5)\n",
        "print(predictions)\n",
        "plt.plot(predictions)\n",
        "\"\"\"\n",
        "\n",
        "#potential alternate loss function\n",
        "#lambda y, rv_y: -rv_y.log_prob(y)\n",
        "\"\"\"\n",
        "lambda y, rv_y: -rv_y.kl_divergence(\n",
        "    y,\n",
        "    rv_y,\n",
        "    name='loss'\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "loss = lambda y, rv_y: tfp.distributions.kl_divergence(\n",
        "    y,\n",
        "    kl,\n",
        "    allow_nan_stats=True,\n",
        "    name=None\n",
        ")\n",
        "\n",
        "#los = los*-1\n",
        "\n",
        "one_step_predictive_dist = tfp.sts.one_step_predictive(\n",
        "    model, observed_time_series, parameter_samples=sample_)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuEePktW9f5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = make_state_space_model(\n",
        "    num_timesteps,\n",
        "    param_vals=None,\n",
        "    initial_state_prior=None,\n",
        "    initial_step=0\n",
        ")\n",
        "\n",
        "\n",
        "tfp.sts.build_factored_variational_loss(\n",
        "    model,\n",
        "    observed_time_series,\n",
        "    init_batch_shape=(),\n",
        "    seed=None,\n",
        "    name=None\n",
        ")\n",
        "\"\"\"\n",
        "model = tf.keras.Sequential([\n",
        "    \n",
        "    tf.keras.Input(shape=(1,13), dtype=x.dtype),\n",
        "    tf.keras.layers.LSTM(8,kernel_initializer='ones', use_bias=False),\n",
        "    #tf.keras.layers.InputLayer(input_shape=(10),dtype=x.dtype),#put a 1 before the 9 later\n",
        "    #tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(400,kernel_initializer='ones', use_bias=False),#check what the shape is it should be 237,9,1 and put LSTM HERE later\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(800,kernel_initializer='ones',use_bias=False),\n",
        "\n",
        "    #goal is to eventually replace the first dense layer with an LSTM layer \n",
        "    #tf.keras.layers.LSTM\n",
        "    #tf.keras.layers.TimeDistributed(Dense(vocabulary)))\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(10, kernel_initializer='ones',use_bias=False,),\n",
        "    tfp.layers.DenseFlipout(512, activation=tf.nn.relu),\n",
        "    #tfp.layers.DenseFlipout(1),\n",
        "])\n",
        "#kalaman filter\n",
        "\n",
        "model = LinearGaussianStateSpaceModel(\n",
        "  num_timesteps=100,\n",
        "  transition_matrix=tfl.LinearOperatorIdentity(ndims),\n",
        "  transition_noise=tfd.MultivariateNormalDiag(\n",
        "   scale_diag=step_std**2 * tf.ones([ndims])),\n",
        "  observation_matrix=tfl.LinearOperatorIdentity(ndims),\n",
        "  observation_noise=tfd.MultivariateNormalDiag(\n",
        "   scale_diag=noise_std**2 * tf.ones([ndims])),\n",
        "  initial_state_prior=tfd.MultivariateNormalDiag(\n",
        "   scale_diag=tf.ones([ndims])))\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#First sucessful build using tf 2.0 with functional API\n",
        "\n",
        "num_inducing_points = 45\n",
        "inputs = tf.keras.Input(shape=(1,10))\n",
        "first  = tf.keras.layers.LSTM(8,kernel_initializer='ones', use_bias=False)(inputs)\n",
        "first  = tf.keras.layers.Dense(8,kernel_initializer='ones', use_bias=False)(first)\n",
        "u = tf.keras.layers.BatchNormalization()(first)\n",
        "u = tf.keras.layers.Dense(800,kernel_initializer='ones',use_bias=False)(u)\n",
        "outputs = tf.keras.layers.Dense(10, activation='sigmoid')(u)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymp_Xa7kcTv1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trhqOpfdboS1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zMCSW3RJaxo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "notes on presentation\n",
        "\n",
        "asks what diversity and inclusion are\n",
        "\n",
        "different priorities or miscommunication may be affected by background\n",
        "\n",
        "\n",
        "diversity; who is on the team\n",
        "\n",
        "culture plays a role in every attitude \n",
        "\n",
        "do i care about your unique perspective\n",
        "\n",
        "do i understand differences\n",
        "\n",
        "diversity is more then just race and ethnicity and age gender race national origin\n",
        "\n",
        "now asking what does it mean to be in a more i diverse community\n",
        "\n",
        "Showing pictures of diversity initiatives from companies\n",
        "\n",
        "now asking how to develop cultural competency"
      ]
    }
  ]
}